{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-05T12:59:35.454333Z",
     "start_time": "2023-10-05T12:57:40.269482600Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from traffic.core import Traffic\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import FloatType,  IntegerType, StringType\n",
    "from joblib import load as load_scaler\n",
    "from elephas.ml_model import ElephasEstimator\n",
    "from src.processing_utils.preprocessing import preprocess_traffic, generate_aux_columns, seconds_till_arrival\n",
    "import random\n",
    "from src.models.fnn import build_sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_PYTHON'] = 'C:/Users/dario/anaconda3/envs/Lufthansa-Arrival-Time-Prediction/python.exe'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'C:/Users/dario/anaconda3/envs/Lufthansa-Arrival-Time-Prediction/python.exe'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LoadData\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "columns = [\n",
    "    \"distance\",\n",
    "    \"altitude\",\n",
    "    \"geoaltitude\",\n",
    "    \"vertical_rate\",\n",
    "    \"groundspeed\",\n",
    "    \"holiday\",\n",
    "    'sec_sin',\n",
    "    'sec_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "    'bearing_sin',\n",
    "    'bearing_cos',\n",
    "    'track_sin',\n",
    "    'track_cos',\n",
    "    'latitude_rad',\n",
    "    'longitude_rad',\n",
    "    'weekday_0',\n",
    "    'weekday_1',\n",
    "    'weekday_2',\n",
    "    'weekday_3',\n",
    "    'weekday_4',\n",
    "    'weekday_5',\n",
    "    'weekday_6',\n",
    "    'seconds_till_arrival',\n",
    "    \"day\",\n",
    "]\n",
    "COLS_TO_SCALE = [\"distance\", \"altitude\", \"geoaltitude\", \"vertical_rate\", \"groundspeed\"]\n",
    "PATH_SCALER = os.path.join(\"..\", \"trained_models\", \"std_scaler_reg_new.bin\")\n",
    "scaler = load_scaler(PATH_SCALER)\n",
    "def load_data_batch_spark(file_batch):\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"distance\", FloatType(), True),\n",
    "        StructField(\"altitude\", FloatType(), True),\n",
    "        StructField(\"geoaltitude\", FloatType(), True),\n",
    "        StructField(\"vertical_rate\", FloatType(), True),\n",
    "        StructField(\"groundspeed\", FloatType(), True),\n",
    "        StructField(\"holiday\", IntegerType(), True),\n",
    "        StructField('sec_sin',FloatType(), True),\n",
    "        StructField('sec_cos',FloatType(), True),\n",
    "        StructField('day_sin',FloatType(), True),\n",
    "        StructField('day_cos',FloatType(), True),\n",
    "        StructField('bearing_sin',FloatType(), True),\n",
    "        StructField('bearing_cos',FloatType(), True),\n",
    "        StructField('track_sin',FloatType(), True),\n",
    "        StructField('track_cos',FloatType(), True),\n",
    "        StructField('latitude_rad',FloatType(), True),\n",
    "        StructField('longitude_rad',FloatType(), True),\n",
    "        StructField('weekday_0',IntegerType(), True),\n",
    "        StructField('weekday_1', IntegerType(), True),\n",
    "        StructField('weekday_2', IntegerType(), True),\n",
    "        StructField('weekday_3', IntegerType(), True),\n",
    "        StructField('weekday_4', IntegerType(), True),\n",
    "        StructField('weekday_5', IntegerType(), True),\n",
    "        StructField('weekday_6', IntegerType(), True),\n",
    "        StructField('seconds_till_arrival', FloatType(), True),\n",
    "        StructField('day', StringType(), True),\n",
    "    ])\n",
    "    print(len(schema), len(columns))\n",
    "    # Placeholder for the final DataFrame\n",
    "    all_flights = None\n",
    "    first_month = True\n",
    "    for file in file_batch:\n",
    "\n",
    "        with h5py.File(file, 'r') as f:\n",
    "            first_day = True\n",
    "\n",
    "            for key in tqdm(list(f.keys())[:5], desc=file):\n",
    "                # Retrieve the data using your existing method\n",
    "                new_flights = Traffic.from_file(file, key=key,\n",
    "                                                parse_dates=[\"day\", \"firstseen\", \"hour\", \"last_position\",\n",
    "                                                             \"lastseen\", \"timestamp\"]).data\n",
    "                if first_day:\n",
    "                    df_flights = new_flights.copy()\n",
    "                    first_day = False\n",
    "                else:\n",
    "                    df_flights = pd.concat([df_flights, new_flights])\n",
    "\n",
    "            df_flights = preprocess_traffic(df_flights, remove_noise=True)\n",
    "            df_flights = generate_aux_columns(df_flights)\n",
    "            df_flights[\"day\"] = df_flights.firstseen.astype(str)\n",
    "            df_flights = df_flights.dropna()\n",
    "            df_flights[\"seconds_till_arrival\"] = seconds_till_arrival(df_flights)\n",
    "            df_flights[COLS_TO_SCALE] =scaler.transform(df_flights[COLS_TO_SCALE])\n",
    "\n",
    "\n",
    "        if first_month:\n",
    "            all_flights = spark.createDataFrame(df_flights[columns], schema=schema)\n",
    "            first_month = False\n",
    "        else:\n",
    "            add = spark.createDataFrame(df_flights[columns], schema=schema)\n",
    "            all_flights = all_flights.union(add)\n",
    "\n",
    "    return all_flights\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:49:49.579800800Z",
     "start_time": "2023-10-05T13:49:49.556345600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "['..\\\\data\\\\raw\\\\Frankfurt_LH_2201.h5',\n '..\\\\data\\\\raw\\\\Frankfurt_LH_2202.h5',\n '..\\\\data\\\\raw\\\\Frankfurt_LH_2203.h5',\n '..\\\\data\\\\raw\\\\Frankfurt_LH_2204.h5',\n '..\\\\data\\\\raw\\\\Frankfurt_LH_2205.h5',\n '..\\\\data\\\\raw\\\\Frankfurt_LH_2206.h5',\n '..\\\\data\\\\raw\\\\Frankfurt_LH_2207.h5',\n '..\\\\data\\\\raw\\\\Frankfurt_LH_2208.h5',\n '..\\\\data\\\\raw\\\\Frankfurt_LH_2209.h5',\n '..\\\\data\\\\raw\\\\Frankfurt_LH_2210.h5',\n '..\\\\data\\\\raw\\\\Frankfurt_LH_2211.h5',\n '..\\\\data\\\\raw\\\\Frankfurt_LH_2212.h5']"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH_DATA = os.path.join(\"..\", \"data\", \"raw\")\n",
    "files = [os.path.join(PATH_DATA,file) for file in os.listdir(PATH_DATA)]\n",
    "files"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:49:51.109555900Z",
     "start_time": "2023-10-05T13:49:51.097494600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 25\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2201.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cad00c8378b84ff29e5de4a5144bf8a6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2202.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f79b521b56b841c580ef5abb06f66c58"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2203.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d98f6eaf8c044848b581adfab0b42130"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2204.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39cd768a692c4baca5487df8b7e9f15e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2205.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68d0f2c030e648c081e257e849c3d390"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2206.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea3937bcb23f4b7d99ebb280e886194f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2207.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89923e90b6a34f77a7a8f9b195e4f61c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2208.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "387dcfbef66c4c35834bdcbd58c9b2bb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2209.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a29977dde8b24dbcb1919dce623fe35f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2210.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1bbbbb3e0c344049a52edaf608ac690"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2211.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1d61e3942ab4afabc7fd5d0cdf39887"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    },
    {
     "data": {
      "text/plain": "..\\data\\raw\\Frankfurt_LH_2212.h5:   0%|          | 0/5 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71bbcdb3138d4a6e87b3ff4ceb2ff574"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:604: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  [(c, t) for (_, c), t in zip(pdf_slice.iteritems(), arrow_types)]\n"
     ]
    }
   ],
   "source": [
    "spark_df = load_data_batch_spark(files)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T14:08:57.445600900Z",
     "start_time": "2023-10-05T13:49:55.575009200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1484.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:288)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$26(FileFormatWriter.scala:277)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:277)\r\n\t... 42 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[43], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mspark_df\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m../data/processed/test_df\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1240\u001B[0m, in \u001B[0;36mDataFrameWriter.csv\u001B[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001B[0m\n\u001B[0;32m   1221\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode(mode)\n\u001B[0;32m   1222\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_opts(\n\u001B[0;32m   1223\u001B[0m     compression\u001B[38;5;241m=\u001B[39mcompression,\n\u001B[0;32m   1224\u001B[0m     sep\u001B[38;5;241m=\u001B[39msep,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1238\u001B[0m     lineSep\u001B[38;5;241m=\u001B[39mlineSep,\n\u001B[0;32m   1239\u001B[0m )\n\u001B[1;32m-> 1240\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcsv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    189\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 190\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    191\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    192\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    324\u001B[0m value \u001B[38;5;241m=\u001B[39m OUTPUT_CONVERTER[\u001B[38;5;28mtype\u001B[39m](answer[\u001B[38;5;241m2\u001B[39m:], gateway_client)\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    330\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n",
      "\u001B[1;31mPy4JJavaError\u001B[0m: An error occurred while calling o1484.csv.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:288)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:851)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$26(FileFormatWriter.scala:277)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:277)\r\n\t... 42 more\r\n"
     ]
    }
   ],
   "source": [
    "spark_df.write.csv(\"../data/processed/test_df\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T14:17:36.875347600Z",
     "start_time": "2023-10-05T14:16:47.222194600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Input, LeakyReLU, Dropout, Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "\n",
    "def build_sequential(\n",
    "        lr: float = 0.001,\n",
    "        input_dims: tuple = (23,),\n",
    "        output_dims: int = 1,\n",
    "        layer_sizes: tuple = (1024, 512, 256),\n",
    "        dropout_rate: float = 0.2,\n",
    "        activation: str = \"softplus\",\n",
    "        loss: str = \"MAE\",\n",
    "):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=input_dims))\n",
    "    for size in layer_sizes:\n",
    "        model.add(Dense(size))\n",
    "        model.add(LeakyReLU(alpha=0.05))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(output_dims, activation=activation))\n",
    "    model.compile(optimizer=Adam(learning_rate=lr), loss=loss)\n",
    "    return model\n",
    "\n",
    "model = build_sequential()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T14:19:16.816067600Z",
     "start_time": "2023-10-05T14:19:16.319792600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "flight_ids = spark_df.select(\"day\").distinct().rdd.flatMap(lambda x: x).collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T14:19:26.998139600Z",
     "start_time": "2023-10-05T14:19:17.687929200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "random.shuffle(flight_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T14:19:53.976358600Z",
     "start_time": "2023-10-05T14:19:53.964828200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "ind_last = int(len(flight_ids)*0.95)\n",
    "flights_train = flight_ids[:ind_last]\n",
    "flights_test = flight_ids[ind_last:]\n",
    "train_df = spark_df.filter(spark_df.day.isin(flights_train))\n",
    "test_df = spark_df.filter(spark_df.day.isin(flights_test))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T14:20:12.331997100Z",
     "start_time": "2023-10-05T14:19:54.690945400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import rand\n",
    "train_df = train_df.orderBy(rand())\n",
    "train_df = train_df.drop(\"day\")\n",
    "feature_columns = [col_name for col_name in train_df.columns if col_name != 'seconds_till_arrival']\n",
    "vec_assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "train_df = vec_assembler.transform(train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T14:21:08.684141100Z",
     "start_time": "2023-10-05T14:21:08.549464600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "df_transform_fin = train_df.select('features','seconds_till_arrival')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T14:21:09.805952100Z",
     "start_time": "2023-10-05T14:21:09.779897700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "ElephasEstimator_f7df5f77f5a0"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "# Set and Serialize Optimizer\n",
    "optimizer_conf = Adam(learning_rate=0.0001)\n",
    "opt_conf = optimizers.serialize(optimizer_conf)\n",
    "\n",
    "# Initialize SparkML Estimator and Get Settings\n",
    "estimator = ElephasEstimator()\n",
    "estimator.setFeaturesCol(\"features\")\n",
    "estimator.setLabelCol(\"seconds_till_arrival\")\n",
    "estimator.set_keras_model_config(model.to_json())\n",
    "estimator.set_num_workers(3)\n",
    "estimator.set_epochs(2)\n",
    "estimator.set_batch_size(128)\n",
    "estimator.set_verbosity(1)\n",
    "estimator.set_optimizer_config(opt_conf)\n",
    "estimator.set_mode(\"synchronous\")\n",
    "estimator.set_loss(\"mae\")\n",
    "estimator.set_categorical_labels(False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T14:30:37.378018100Z",
     "start_time": "2023-10-05T14:30:37.356218Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Fit model\n"
     ]
    }
   ],
   "source": [
    "fitted_model = estimator.fit(df_transform_fin)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-05T14:30:40.428009900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "df_test = test_df.toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:33:28.191938600Z",
     "start_time": "2023-10-05T13:33:27.935784900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "X = df_test.drop(columns=[\"seconds_till_arrival\", \"day\"])\n",
    "y = df_test.seconds_till_arrival"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:33:32.875942100Z",
     "start_time": "2023-10-05T13:33:32.856057200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model = build_sequential(input_dims=(23,))\n",
    "model.set_weights(fitted_model.weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:33:35.933994400Z",
     "start_time": "2023-10-05T13:33:35.842480600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "481/481 [==============================] - 3s 5ms/step - loss: 547.7393\n"
     ]
    },
    {
     "data": {
      "text/plain": "547.7393188476562"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X,y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:33:39.960367700Z",
     "start_time": "2023-10-05T13:33:36.999677200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "       distance  altitude  geoaltitude  vertical_rate  groundspeed  holiday  \\\n0     -0.479459 -2.106921    -2.088638       0.835945    -2.582180        0   \n1     -0.479341 -2.082564    -2.066489       0.759652    -2.482535        0   \n2     -0.479102 -2.035880    -2.022192       0.708790    -2.313140        0   \n3     -0.478460 -1.926273    -1.919503       1.191978    -1.725240        0   \n4     -0.477986 -1.778101    -1.776544       1.446288    -1.466165        0   \n...         ...       ...          ...            ...          ...      ...   \n15356 -0.632407 -2.102861    -2.098705      -0.283017    -2.671859        0   \n15357 -0.632491 -2.104891    -2.098705      -0.283017    -2.671859        0   \n15358 -0.632633 -2.110981    -2.104746      -0.232155    -2.671859        0   \n15359 -0.632981 -2.119099    -2.112800      -0.283017    -2.681824        0   \n15360 -0.633394 -2.133308    -2.126894      -0.283017    -2.691788        0   \n\n        sec_sin   sec_cos   day_sin   day_cos  ...  longitude_rad  weekday_0  \\\n0      0.991264  0.131896 -0.493776  0.869589  ...       0.169794          0   \n1      0.991359  0.131175 -0.493776  0.869589  ...       0.169994          0   \n2      0.991540  0.129805 -0.493776  0.869589  ...       0.170402          0   \n3      0.991931  0.126776 -0.493776  0.869589  ...       0.171522          0   \n4      0.992242  0.124323 -0.493776  0.869589  ...       0.172607          0   \n...         ...       ...       ...       ...  ...            ...        ...   \n15356 -0.946156  0.323711 -0.432776  0.901502  ...       0.150851          1   \n15357 -0.946109  0.323849 -0.432776  0.901502  ...       0.150816          1   \n15358 -0.945991  0.324193 -0.432776  0.901502  ...       0.150758          1   \n15359 -0.945802  0.324743 -0.432776  0.901502  ...       0.150614          1   \n15360 -0.945471  0.325706 -0.432776  0.901502  ...       0.150444          1   \n\n       weekday_1  weekday_2  weekday_3  weekday_4  weekday_5  weekday_6  \\\n0              0          0          1          0          0          0   \n1              0          0          1          0          0          0   \n2              0          0          1          0          0          0   \n3              0          0          1          0          0          0   \n4              0          0          1          0          0          0   \n...          ...        ...        ...        ...        ...        ...   \n15356          0          0          0          0          0          0   \n15357          0          0          0          0          0          0   \n15358          0          0          0          0          0          0   \n15359          0          0          0          0          0          0   \n15360          0          0          0          0          0          0   \n\n       seconds_till_arrival                        day  \n0                    2465.0  2022-12-01 05:29:22+00:00  \n1                    2455.0  2022-12-01 05:29:22+00:00  \n2                    2436.0  2022-12-01 05:29:22+00:00  \n3                    2394.0  2022-12-01 05:29:22+00:00  \n4                    2360.0  2022-12-01 05:29:22+00:00  \n...                     ...                        ...  \n15356                  81.0  2022-12-05 17:30:40+00:00  \n15357                  79.0  2022-12-05 17:30:40+00:00  \n15358                  74.0  2022-12-05 17:30:40+00:00  \n15359                  66.0  2022-12-05 17:30:40+00:00  \n15360                  52.0  2022-12-05 17:30:40+00:00  \n\n[15361 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>distance</th>\n      <th>altitude</th>\n      <th>geoaltitude</th>\n      <th>vertical_rate</th>\n      <th>groundspeed</th>\n      <th>holiday</th>\n      <th>sec_sin</th>\n      <th>sec_cos</th>\n      <th>day_sin</th>\n      <th>day_cos</th>\n      <th>...</th>\n      <th>longitude_rad</th>\n      <th>weekday_0</th>\n      <th>weekday_1</th>\n      <th>weekday_2</th>\n      <th>weekday_3</th>\n      <th>weekday_4</th>\n      <th>weekday_5</th>\n      <th>weekday_6</th>\n      <th>seconds_till_arrival</th>\n      <th>day</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.479459</td>\n      <td>-2.106921</td>\n      <td>-2.088638</td>\n      <td>0.835945</td>\n      <td>-2.582180</td>\n      <td>0</td>\n      <td>0.991264</td>\n      <td>0.131896</td>\n      <td>-0.493776</td>\n      <td>0.869589</td>\n      <td>...</td>\n      <td>0.169794</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2465.0</td>\n      <td>2022-12-01 05:29:22+00:00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.479341</td>\n      <td>-2.082564</td>\n      <td>-2.066489</td>\n      <td>0.759652</td>\n      <td>-2.482535</td>\n      <td>0</td>\n      <td>0.991359</td>\n      <td>0.131175</td>\n      <td>-0.493776</td>\n      <td>0.869589</td>\n      <td>...</td>\n      <td>0.169994</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2455.0</td>\n      <td>2022-12-01 05:29:22+00:00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.479102</td>\n      <td>-2.035880</td>\n      <td>-2.022192</td>\n      <td>0.708790</td>\n      <td>-2.313140</td>\n      <td>0</td>\n      <td>0.991540</td>\n      <td>0.129805</td>\n      <td>-0.493776</td>\n      <td>0.869589</td>\n      <td>...</td>\n      <td>0.170402</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2436.0</td>\n      <td>2022-12-01 05:29:22+00:00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.478460</td>\n      <td>-1.926273</td>\n      <td>-1.919503</td>\n      <td>1.191978</td>\n      <td>-1.725240</td>\n      <td>0</td>\n      <td>0.991931</td>\n      <td>0.126776</td>\n      <td>-0.493776</td>\n      <td>0.869589</td>\n      <td>...</td>\n      <td>0.171522</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2394.0</td>\n      <td>2022-12-01 05:29:22+00:00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.477986</td>\n      <td>-1.778101</td>\n      <td>-1.776544</td>\n      <td>1.446288</td>\n      <td>-1.466165</td>\n      <td>0</td>\n      <td>0.992242</td>\n      <td>0.124323</td>\n      <td>-0.493776</td>\n      <td>0.869589</td>\n      <td>...</td>\n      <td>0.172607</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2360.0</td>\n      <td>2022-12-01 05:29:22+00:00</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15356</th>\n      <td>-0.632407</td>\n      <td>-2.102861</td>\n      <td>-2.098705</td>\n      <td>-0.283017</td>\n      <td>-2.671859</td>\n      <td>0</td>\n      <td>-0.946156</td>\n      <td>0.323711</td>\n      <td>-0.432776</td>\n      <td>0.901502</td>\n      <td>...</td>\n      <td>0.150851</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>81.0</td>\n      <td>2022-12-05 17:30:40+00:00</td>\n    </tr>\n    <tr>\n      <th>15357</th>\n      <td>-0.632491</td>\n      <td>-2.104891</td>\n      <td>-2.098705</td>\n      <td>-0.283017</td>\n      <td>-2.671859</td>\n      <td>0</td>\n      <td>-0.946109</td>\n      <td>0.323849</td>\n      <td>-0.432776</td>\n      <td>0.901502</td>\n      <td>...</td>\n      <td>0.150816</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>79.0</td>\n      <td>2022-12-05 17:30:40+00:00</td>\n    </tr>\n    <tr>\n      <th>15358</th>\n      <td>-0.632633</td>\n      <td>-2.110981</td>\n      <td>-2.104746</td>\n      <td>-0.232155</td>\n      <td>-2.671859</td>\n      <td>0</td>\n      <td>-0.945991</td>\n      <td>0.324193</td>\n      <td>-0.432776</td>\n      <td>0.901502</td>\n      <td>...</td>\n      <td>0.150758</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>74.0</td>\n      <td>2022-12-05 17:30:40+00:00</td>\n    </tr>\n    <tr>\n      <th>15359</th>\n      <td>-0.632981</td>\n      <td>-2.119099</td>\n      <td>-2.112800</td>\n      <td>-0.283017</td>\n      <td>-2.681824</td>\n      <td>0</td>\n      <td>-0.945802</td>\n      <td>0.324743</td>\n      <td>-0.432776</td>\n      <td>0.901502</td>\n      <td>...</td>\n      <td>0.150614</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>66.0</td>\n      <td>2022-12-05 17:30:40+00:00</td>\n    </tr>\n    <tr>\n      <th>15360</th>\n      <td>-0.633394</td>\n      <td>-2.133308</td>\n      <td>-2.126894</td>\n      <td>-0.283017</td>\n      <td>-2.691788</td>\n      <td>0</td>\n      <td>-0.945471</td>\n      <td>0.325706</td>\n      <td>-0.432776</td>\n      <td>0.901502</td>\n      <td>...</td>\n      <td>0.150444</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>52.0</td>\n      <td>2022-12-05 17:30:40+00:00</td>\n    </tr>\n  </tbody>\n</table>\n<p>15361 rows  25 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:38:54.124170Z",
     "start_time": "2023-10-05T13:38:54.091923500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dario\\anaconda3\\envs\\Lufthansa-Arrival-Time-Prediction\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:114: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT()\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "df_train = train_df.toPandas()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:39:22.327653Z",
     "start_time": "2023-10-05T13:39:11.582619600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "        distance  altitude  geoaltitude  vertical_rate  groundspeed  holiday  \\\n0      -0.354216  0.899144     0.834975       0.022155     0.626362        0   \n1      -0.459042  0.899144     0.804772       0.047585     0.437038        0   \n2      -0.281404  0.738793     0.669867       0.276464     0.347358        0   \n3      -0.067310  0.576413     0.494692       0.022155     0.845579        0   \n4      -0.136666  0.744883     0.752421       0.403619     0.476895        0   \n...          ...       ...          ...            ...          ...      ...   \n328532 -0.124803  0.625127     0.555097       0.123878     0.626362        0   \n328533  2.525172  0.077094     0.041653       0.607067     1.373693        0   \n328534 -0.175356  0.736764     0.675908       0.022155     0.187927        0   \n328535 -0.520547  0.623098     0.557111      -1.198531     0.367287        0   \n328536 -0.625844 -1.974987    -1.963800      -0.359310    -2.372927        0   \n\n         sec_sin   sec_cos   day_sin   day_cos  ...  longitude_rad  weekday_0  \\\n0       0.535213 -0.844717 -0.493776  0.869589  ...       0.179076          0   \n1      -0.876482  0.481435 -0.463550  0.886071  ...       0.191889          0   \n2       0.954479 -0.298277 -0.432776  0.901502  ...       0.011908          1   \n3       0.759413  0.650609 -0.448229  0.893919  ...       0.001987          0   \n4       0.994680  0.103010 -0.478734  0.877960  ...      -0.057331          0   \n...          ...       ...       ...       ...  ...            ...        ...   \n328532 -0.999828 -0.018543 -0.493776  0.869589  ...       0.079938          0   \n328533 -0.628189  0.778060 -0.463550  0.886071  ...      -1.210711          0   \n328534 -0.984490 -0.175438 -0.478734  0.877960  ...       0.038969          0   \n328535 -0.476775 -0.879025 -0.448229  0.893919  ...       0.100285          0   \n328536 -0.996150 -0.087663 -0.493776  0.869589  ...       0.145336          0   \n\n        weekday_1  weekday_2  weekday_3  weekday_4  weekday_5  weekday_6  \\\n0               0          0          1          0          0          0   \n1               0          0          0          0          1          0   \n2               0          0          0          0          0          0   \n3               0          0          0          0          0          1   \n4               0          0          0          1          0          0   \n...           ...        ...        ...        ...        ...        ...   \n328532          0          0          1          0          0          0   \n328533          0          0          0          0          1          0   \n328534          0          0          0          1          0          0   \n328535          0          0          0          0          0          1   \n328536          0          0          1          0          0          0   \n\n        seconds_till_arrival  \\\n0                     3237.0   \n1                     2349.0   \n2                     3716.0   \n3                     5006.0   \n4                     4843.0   \n...                      ...   \n328532                4698.0   \n328533               24571.0   \n328534                4906.0   \n328535                1395.0   \n328536                 202.0   \n\n                                                 features  \n0       [-0.3542162775993347, 0.8991439938545227, 0.83...  \n1       [-0.459041953086853, 0.8991439938545227, 0.804...  \n2       [-0.2814040780067444, 0.7387934923171997, 0.66...  \n3       [-0.06731020659208298, 0.5764132142066956, 0.4...  \n4       [-0.13666552305221558, 0.7448827028274536, 0.7...  \n...                                                   ...  \n328532  [-0.12480297684669495, 0.6251272559165955, 0.5...  \n328533  [2.5251717567443848, 0.07709381729364395, 0.04...  \n328534  [-0.17535614967346191, 0.7367637157440186, 0.6...  \n328535  [-0.5205473303794861, 0.6230975389480591, 0.55...  \n328536  [-0.6258440017700195, -1.9749870300292969, -1....  \n\n[328537 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>distance</th>\n      <th>altitude</th>\n      <th>geoaltitude</th>\n      <th>vertical_rate</th>\n      <th>groundspeed</th>\n      <th>holiday</th>\n      <th>sec_sin</th>\n      <th>sec_cos</th>\n      <th>day_sin</th>\n      <th>day_cos</th>\n      <th>...</th>\n      <th>longitude_rad</th>\n      <th>weekday_0</th>\n      <th>weekday_1</th>\n      <th>weekday_2</th>\n      <th>weekday_3</th>\n      <th>weekday_4</th>\n      <th>weekday_5</th>\n      <th>weekday_6</th>\n      <th>seconds_till_arrival</th>\n      <th>features</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.354216</td>\n      <td>0.899144</td>\n      <td>0.834975</td>\n      <td>0.022155</td>\n      <td>0.626362</td>\n      <td>0</td>\n      <td>0.535213</td>\n      <td>-0.844717</td>\n      <td>-0.493776</td>\n      <td>0.869589</td>\n      <td>...</td>\n      <td>0.179076</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3237.0</td>\n      <td>[-0.3542162775993347, 0.8991439938545227, 0.83...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.459042</td>\n      <td>0.899144</td>\n      <td>0.804772</td>\n      <td>0.047585</td>\n      <td>0.437038</td>\n      <td>0</td>\n      <td>-0.876482</td>\n      <td>0.481435</td>\n      <td>-0.463550</td>\n      <td>0.886071</td>\n      <td>...</td>\n      <td>0.191889</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2349.0</td>\n      <td>[-0.459041953086853, 0.8991439938545227, 0.804...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.281404</td>\n      <td>0.738793</td>\n      <td>0.669867</td>\n      <td>0.276464</td>\n      <td>0.347358</td>\n      <td>0</td>\n      <td>0.954479</td>\n      <td>-0.298277</td>\n      <td>-0.432776</td>\n      <td>0.901502</td>\n      <td>...</td>\n      <td>0.011908</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3716.0</td>\n      <td>[-0.2814040780067444, 0.7387934923171997, 0.66...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.067310</td>\n      <td>0.576413</td>\n      <td>0.494692</td>\n      <td>0.022155</td>\n      <td>0.845579</td>\n      <td>0</td>\n      <td>0.759413</td>\n      <td>0.650609</td>\n      <td>-0.448229</td>\n      <td>0.893919</td>\n      <td>...</td>\n      <td>0.001987</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>5006.0</td>\n      <td>[-0.06731020659208298, 0.5764132142066956, 0.4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.136666</td>\n      <td>0.744883</td>\n      <td>0.752421</td>\n      <td>0.403619</td>\n      <td>0.476895</td>\n      <td>0</td>\n      <td>0.994680</td>\n      <td>0.103010</td>\n      <td>-0.478734</td>\n      <td>0.877960</td>\n      <td>...</td>\n      <td>-0.057331</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4843.0</td>\n      <td>[-0.13666552305221558, 0.7448827028274536, 0.7...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>328532</th>\n      <td>-0.124803</td>\n      <td>0.625127</td>\n      <td>0.555097</td>\n      <td>0.123878</td>\n      <td>0.626362</td>\n      <td>0</td>\n      <td>-0.999828</td>\n      <td>-0.018543</td>\n      <td>-0.493776</td>\n      <td>0.869589</td>\n      <td>...</td>\n      <td>0.079938</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4698.0</td>\n      <td>[-0.12480297684669495, 0.6251272559165955, 0.5...</td>\n    </tr>\n    <tr>\n      <th>328533</th>\n      <td>2.525172</td>\n      <td>0.077094</td>\n      <td>0.041653</td>\n      <td>0.607067</td>\n      <td>1.373693</td>\n      <td>0</td>\n      <td>-0.628189</td>\n      <td>0.778060</td>\n      <td>-0.463550</td>\n      <td>0.886071</td>\n      <td>...</td>\n      <td>-1.210711</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>24571.0</td>\n      <td>[2.5251717567443848, 0.07709381729364395, 0.04...</td>\n    </tr>\n    <tr>\n      <th>328534</th>\n      <td>-0.175356</td>\n      <td>0.736764</td>\n      <td>0.675908</td>\n      <td>0.022155</td>\n      <td>0.187927</td>\n      <td>0</td>\n      <td>-0.984490</td>\n      <td>-0.175438</td>\n      <td>-0.478734</td>\n      <td>0.877960</td>\n      <td>...</td>\n      <td>0.038969</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4906.0</td>\n      <td>[-0.17535614967346191, 0.7367637157440186, 0.6...</td>\n    </tr>\n    <tr>\n      <th>328535</th>\n      <td>-0.520547</td>\n      <td>0.623098</td>\n      <td>0.557111</td>\n      <td>-1.198531</td>\n      <td>0.367287</td>\n      <td>0</td>\n      <td>-0.476775</td>\n      <td>-0.879025</td>\n      <td>-0.448229</td>\n      <td>0.893919</td>\n      <td>...</td>\n      <td>0.100285</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1395.0</td>\n      <td>[-0.5205473303794861, 0.6230975389480591, 0.55...</td>\n    </tr>\n    <tr>\n      <th>328536</th>\n      <td>-0.625844</td>\n      <td>-1.974987</td>\n      <td>-1.963800</td>\n      <td>-0.359310</td>\n      <td>-2.372927</td>\n      <td>0</td>\n      <td>-0.996150</td>\n      <td>-0.087663</td>\n      <td>-0.493776</td>\n      <td>0.869589</td>\n      <td>...</td>\n      <td>0.145336</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>202.0</td>\n      <td>[-0.6258440017700195, -1.9749870300292969, -1....</td>\n    </tr>\n  </tbody>\n</table>\n<p>328537 rows  25 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-05T13:39:28.177013900Z",
     "start_time": "2023-10-05T13:39:28.058979900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
